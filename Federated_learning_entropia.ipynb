{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joyboy129/federated/blob/main/Federated_learning_entropia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN8P0AnTnAhh"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ZrGitA_KnRO0"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "QLyJIaLlERJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c530c5a8-45af-4402-d43e-1a469c686e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BKyHkMxKHfV"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afc6bc6f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYc9s0OrF6Av"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/first_file.csv\", sep=',', encoding='latin-1')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "151ebe1e"
      },
      "outputs": [],
      "source": [
        "# Run for Bigdata only\n",
        "# df.rename(columns={\"ElectronÃ©gativitÃ©\": \"Electronégativité\"}, inplace=True)\n",
        "# df.drop(\"Unnamed: 0\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81QhaunIGuFL"
      },
      "outputs": [],
      "source": [
        "df.dtypes\n",
        "# df=df.iloc[:50000]\n",
        "# Run for small data\n",
        "df.rename(columns={\"ElectronÃ©gativitÃ©\": \"Electronégativité\"}, inplace=True)\n",
        "\n",
        "df.columns\n",
        "df.drop(['Alloy', 'Longue Phase', 'Ag', 'Al', 'Cd', 'Ga', 'Mg', 'Pt', 'Sb',\n",
        "       'Sn', 'Ti', 'Zn', 'Zr', 'B', 'Ca', 'Ce', 'Co', 'Cr', 'Cu', 'Er', 'Fe',\n",
        "       'In', 'La', 'Li', 'Mn', 'Mo', 'Nd', 'Ni', 'Re', 'Ru', 'Sm', 'V', 'Yb',\n",
        "       'Au', 'Be', 'Bi', 'Pb', 'Nb', 'Si', 'Dy', 'Ge', 'Hf', 'W', 'Pd', 'Sr',\n",
        "       'Na', 'Y', 'Ta', 'Gd', 'Rh', 'P', 'C', 'Ho', 'Pr', 'N', 'Tb', 'Sc',\n",
        "       'Tm', 'Lu', 'Ir', 'Os', 'As', 'Tc'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed06a5b1"
      },
      "outputs": [],
      "source": [
        "# client1 = 25\n",
        "# client2 = 25\n",
        "# client3 = 25\n",
        "# client4 = 25\n",
        "df[\"Etat d'entropie\"] = df[\"Etat d'entropie\"].map({'H': 1, 'L': 0})\n",
        "df[\"Etat d'entropie\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgHSHiHhRT9K"
      },
      "outputs": [],
      "source": [
        "column_names=list(df.columns)\n",
        "print(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d507ae97"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# client1 = int((client1/100) *df.shape[0])\n",
        "# client2 = int((client2/100) *df.shape[0])\n",
        "# client3 = int((client3/100) *df.shape[0])\n",
        "# client4 = df.shape[0] - client1 - client2 - client3\n",
        "# my_list = [1] * client1 + [2] * client2 + [3] * client3 + [4] * client4\n",
        "# random.shuffle(my_list)\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "client_distributions = {\n",
        "    1: {'percentage': 0.25, 'distribution': [0.60, 0.40]},\n",
        "    2: {'percentage': 0.25, 'distribution': [0.50, 0.50]},\n",
        "    3: {'percentage': 0.25, 'distribution': [0.70, 0.30]},\n",
        "    4: {'percentage': 0.25, 'distribution': [0.80, 0.20]},\n",
        "}\n",
        "def distribute_dataframe(df, client_distributions):\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Separate the dataframe into 0s and 1s\n",
        "    df0 = df[df['Etat d\\'entropie'] == 0]\n",
        "    df1 = df[df['Etat d\\'entropie'] == 1]\n",
        "\n",
        "    # Perform undersampling\n",
        "    # if len(df0) > len(df1):\n",
        "    #     df0 = df0.sample(len(df1))\n",
        "    # else:\n",
        "    #     df1 = df1.sample(len(df0))\n",
        "\n",
        "    # Combine back to a single dataframe\n",
        "\n",
        "    distributed_df = pd.DataFrame()\n",
        "\n",
        "    for client_id, client_info in client_distributions.items():\n",
        "        client_percentage = client_info['percentage']\n",
        "        class_distribution = client_info['distribution']\n",
        "\n",
        "        # Calculate the exact number of rows for this client\n",
        "        n_rows = int(client_percentage * len(df))\n",
        "\n",
        "        # Calculate desired number of samples for each class\n",
        "        n0 = int(class_distribution[0] * n_rows)\n",
        "        n1 = n_rows - n0\n",
        "\n",
        "        # Check if there are enough samples for each class\n",
        "        if n0 > len(df0):\n",
        "            n0 = len(df0)\n",
        "        if n1 > len(df1):\n",
        "            n1 = len(df1)\n",
        "\n",
        "        # Sample the rows for this client\n",
        "        tmp_df0 = df0.sample(n=n0, random_state=42)\n",
        "        tmp_df1 = df1.sample(n=n1, random_state=42)\n",
        "\n",
        "        # Drop these rows from the original dataframes\n",
        "        df0 = df0.drop(tmp_df0.index)\n",
        "        df1 = df1.drop(tmp_df1.index)\n",
        "\n",
        "        # Add the client id and combine the samples\n",
        "        tmp_df = pd.concat([tmp_df0, tmp_df1])\n",
        "        tmp_df['id'] = client_id\n",
        "\n",
        "        distributed_df = pd.concat([distributed_df, tmp_df])\n",
        "\n",
        "\n",
        "    return distributed_df\n",
        "\n",
        "df =distribute_dataframe(df, client_distributions)\n",
        "# Create a copy of the data excluding the specified column\n",
        "\n",
        "# Prepare the 'id' column\n",
        "# ids = np.random.choice([1,2,3,4], size=len(df), p=list(id_percentages.values()))\n",
        "# df['id'] = ids\n",
        "\n",
        "# # Prepare the \"Etat d'entropie\" column\n",
        "# for id_val, percentages in entropy_percentages.items():\n",
        "#     size = len(df[df['id'] == id_val])\n",
        "#     entropy_values = np.random.choice([0,1], size=size, p=list(percentages.values()))\n",
        "#     df.loc[df['id'] == id_val, 'Etat d\\'entropie'] = entropy_values\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux2gg2btSYl1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize_columns(df, columns_to_normalize):\n",
        "    # Create a copy of the original dataframe\n",
        "    normalized_df = df.copy()\n",
        "\n",
        "    # Select the columns to normalize\n",
        "    data_to_normalize = normalized_df[columns_to_normalize]\n",
        "\n",
        "    # Create an instance of the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler to the selected columns\n",
        "    scaler.fit(data_to_normalize)\n",
        "\n",
        "    # Normalize the selected columns\n",
        "    normalized_data = scaler.transform(data_to_normalize)\n",
        "\n",
        "    # Update the normalized values in the dataframe\n",
        "    normalized_df[columns_to_normalize] = normalized_data\n",
        "\n",
        "    return normalized_df\n",
        "df=normalize_columns(df,['A', 'delta', 'TM', 'DELTA_TM', 'Electronégativité', 'Delta_Electro-', 'SID', 'VEC', 'Delta VEC', 'K', 'Delta K'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Wgpj-2PLJF10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def pair_plot(df):\n",
        "  etat_entropie_values = [0, 1]\n",
        "\n",
        "# Plotting the density plots\n",
        "  id_values = [1, 2, 3,4]\n",
        "\n",
        "# Plotting the density plots\n",
        "  num_rows = 11\n",
        "  num_cols = 4\n",
        "  num_plots = num_rows * num_cols\n",
        "\n",
        "  fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 36))\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  for i, col in enumerate(df.columns):\n",
        "    if col != 'id' and col != 'Etat d\\'entropie':\n",
        "        for j, id_value in enumerate(id_values):\n",
        "            ax = axes[(i * len(id_values)) + j]\n",
        "            for etat_entropie in [0, 1]:\n",
        "                filtered_data = df[df['Etat d\\'entropie'] == etat_entropie]\n",
        "                if not filtered_data.empty:\n",
        "                    filtered_data[col].plot(kind='density', label=f'Etat Entropie = {etat_entropie}', ax=ax)\n",
        "            filtered_data = df[(df['Etat d\\'entropie'] == 0) | (df['Etat d\\'entropie'] == 1)]\n",
        "            if not filtered_data.empty:\n",
        "                filtered_data[col].plot(kind='density', label='All Data', ax=ax, linestyle='--')\n",
        "            ax.set_title(f'Density Plot - {col}, id = {id_value}')\n",
        "            ax.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "pair_plot(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "njVIi3KX-fI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WZgRifhkP22"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a pairplot\n",
        "for i in [1,2,3,4]:\n",
        "  # pair_plot = sns.pairplot(df[df['id'] == i], hue='Etat d\\'entropie')\n",
        "  # pair_plot.savefig(\"/content/drive/MyDrive/Figures/Pairplots/pairplot\"+str(i)+\".png\")\n",
        "\n",
        "\n",
        "\n",
        "# Save the plot as a PNG file\n",
        "# pair_plot.savefig(\"pairplot.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fa330b2"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe478e94"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "np.random.seed(0)\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rwx2ugffeOW"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5074452c"
      },
      "outputs": [],
      "source": [
        "client_id_colname = 'id' # the column that represents client ID\n",
        "SHUFFLE_BUFFER = 100\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ce0e9f1"
      },
      "outputs": [],
      "source": [
        "# split client id into train and test clients\n",
        "train_frac = 0.9\n",
        "client_ids = df[client_id_colname].unique()\n",
        "train_client_ids = [1,2,3]\n",
        "test_client_ids = [4]\n",
        "print(client_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSJF2sfdNHhr"
      },
      "outputs": [],
      "source": [
        "train_client_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dbba4e1"
      },
      "outputs": [],
      "source": [
        "def create_tf_dataset_for_client_fn(client_id):\n",
        "  # a function which takes a client_id and returns a\n",
        "  # tf.data.Dataset for that client\n",
        "    client_data = df[df[client_id_colname] == client_id]\n",
        "    client_data = client_data.drop(columns=[\"id\"])\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))\n",
        "    dataset = dataset.shuffle(SHUFFLE_BUFFER).batch(1).repeat(NUM_EPOCHS)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e62dd808"
      },
      "outputs": [],
      "source": [
        "train_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
        "        client_ids=train_client_ids,\n",
        "        serializable_dataset_fn=create_tf_dataset_for_client_fn\n",
        "    )\n",
        "\n",
        "#\n",
        "test_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
        "        client_ids=test_client_ids,\n",
        "        serializable_dataset_fn=create_tf_dataset_for_client_fn\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efba9238"
      },
      "outputs": [],
      "source": [
        "train_data.element_type_structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be904144"
      },
      "outputs": [],
      "source": [
        "example_dataset = train_data.create_tf_dataset_for_client(\n",
        "    train_data.client_ids[0])\n",
        "\n",
        "example_element = next(iter(example_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0814bb6"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 4\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER = 1000\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "        def batch_format_fn(element):\n",
        "            output = tf.concat([\n",
        "                tf.expand_dims(element['Electronégativité'],1)\n",
        "                ,tf.expand_dims(element['Delta_Electro-'],1)\n",
        "                ,tf.expand_dims(element['SID'],1)\n",
        "                ,tf.expand_dims(element['VEC'],1)\n",
        "                ,tf.expand_dims(element['Delta VEC'],1)\n",
        "                ,tf.expand_dims(element[ 'K'],1)\n",
        "                ,tf.expand_dims(element['Delta K'],1)\n",
        "                ,tf.expand_dims(element['A'],1)\n",
        "                ,tf.expand_dims(element['delta'],1)\n",
        "                ,tf.expand_dims(element['TM'],1)\n",
        "                ,tf.expand_dims(element['DELTA_TM'],1)], axis=1)\n",
        "            return collections.OrderedDict(x= tf.reshape(output, [1, 11] )\n",
        "                                           ,\n",
        "                                           y= tf.reshape(element[\"Etat d'entropie\"], [1,1]) )\n",
        "\n",
        "        return dataset.map(batch_format_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fc13188"
      },
      "outputs": [],
      "source": [
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(preprocessed_example_dataset)))\n",
        "\n",
        "sample_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ebebf42"
      },
      "outputs": [],
      "source": [
        "def make_federated_data(client_data, client_ids):\n",
        "    return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5e10e72"
      },
      "outputs": [],
      "source": [
        "sample_clients = train_data.client_ids[0:NUM_CLIENTS]\n",
        "sample_clients_test=test_data.client_ids[0:NUM_CLIENTS]\n",
        "federated_train_data = make_federated_data(train_data, sample_clients)\n",
        "\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "622e3bd9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_keras_model():\n",
        "    return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Reshape(target_shape=(11, 1), input_shape=(11,)),\n",
        "    # Reshape input to (11, 1)\n",
        "    tf.keras.layers.Conv1D(20, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=8),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(15, kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dense(10, kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dense(5, kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dense(1, kernel_initializer='lecun_normal', activation='relu')\n",
        "    ])\n",
        "# RNN\n",
        "# def create_keras_model():\n",
        "#     return tf.keras.models.Sequential([\n",
        "#         tf.keras.layers.Reshape(target_shape=(11, 1), input_shape=(11,)),\n",
        "#         tf.keras.layers.SimpleRNN(32),\n",
        "#         tf.keras.layers.Dense(85, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Dense(60, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Dense(25, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Softmax(),\n",
        "#     ])\n",
        "# def create_keras_model():\n",
        "#     return tf.keras.models.Sequential([\n",
        "#         tf.keras.layers.Reshape(target_shape=(11, 1), input_shape=(11,)),\n",
        "#         tf.keras.layers.LSTM(32),\n",
        "#         tf.keras.layers.Dense(85, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Dense(60, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Dense(25, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='selu'),\n",
        "#     ])\n",
        "# def create_keras_model():\n",
        "#     return tf.keras.models.Sequential([\n",
        "#         tf.keras.layers.Reshape(target_shape=(11, 1), input_shape=(11,)),\n",
        "#         tf.keras.layers.GRU(32),\n",
        "#         tf.keras.layers.Dense(30, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Dense(25, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Dense(1, kernel_initializer='zeros'),\n",
        "#         tf.keras.layers.Softmax(),\n",
        "#     ])\n",
        "\n",
        "model=create_keras_model()\n",
        "tf.keras.utils.plot_model(\n",
        "model,\n",
        "to_file=\"model_ann.png\",\n",
        "show_shapes=True,\n",
        "show_dtype=False,\n",
        "show_layer_names=True,\n",
        "rankdir=\"TB\",\n",
        "expand_nested=True,\n",
        "dpi=96,\n",
        "layer_range=None,\n",
        "show_layer_activations=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c43084e9"
      },
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "  # We _must_ create a new model here, and _not_ capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "    keras_model = create_keras_model()\n",
        "    return tff.learning.models.from_keras_model(\n",
        "    keras_model=keras_model,\n",
        "    input_spec=preprocessed_example_dataset.element_spec,\n",
        "    loss=tf.keras.losses.Poisson(reduction=\"auto\", name=\"poisson\"),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.SparseCategoricalAccuracy(),\n",
        "        tf.keras.metrics.TruePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.FalseNegatives()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aef5fc54"
      },
      "outputs": [],
      "source": [
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a224b72"
      },
      "outputs": [],
      "source": [
        "str(iterative_process.initialize.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62ad6dcc"
      },
      "outputs": [],
      "source": [
        "state = iterative_process.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrSYpAAEdu8V"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a601363"
      },
      "outputs": [],
      "source": [
        "state, metrics = iterative_process.next(state, federated_train_data)\n",
        "print('round  1, metrics={}'.format(metrics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f339718a"
      },
      "outputs": [],
      "source": [
        "NUM_ROUNDS = 4\n",
        "\n",
        "acc = []\n",
        "for round_num in range(2, NUM_ROUNDS):\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    print('round {:2d}, metrics={}'.format(round_num, metrics))\n",
        "    acc.append(float( metrics['client_work']['train']['sparse_categorical_accuracy']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48b53ef8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Assuming you have an array named acc\n",
        "\n",
        "# Plotting the array\n",
        "plt.plot(acc)\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Plot')\n",
        "plt.show()\n",
        "\n",
        "# Creating a boxplot of the array\n",
        "plt.boxplot(acc)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Boxplot')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fc679f4"
      },
      "outputs": [],
      "source": [
        "max(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl85nNmZvnIP"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "logdir = \"/tmp/logs/scalars/training/\"\n",
        "try:\n",
        "  tf.io.gfile.rmtree(logdir)  # delete any previous results\n",
        "except tf.errors.NotFoundError as e:\n",
        "  pass # Ignore if the directory didn't previously exist.\n",
        "summary_writer = tf.summary.create_file_writer(logdir)\n",
        "train_state = iterative_process.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IQFL5eywsI1"
      },
      "outputs": [],
      "source": [
        "rm -rf ./logs/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRKqr_-rw82d"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EEC7bJNvj8H"
      },
      "source": [
        "Evaluation process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dwSZy3Ry00A"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a model function\n",
        "\n",
        "# Use this model function to create a training process\n",
        "evaluation_process = tff.learning.algorithms.build_fed_eval(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BQtDv8y3ovr"
      },
      "outputs": [],
      "source": [
        "print(evaluation_process.next.type_signature.formatted_representation())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0XF_HC56-db"
      },
      "outputs": [],
      "source": [
        "evaluation_state = evaluation_process.initialize()\n",
        "model_weights = iterative_process.get_model_weights(state)\n",
        "evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siKhDxbY7BVP"
      },
      "outputs": [],
      "source": [
        "evaluation_output = evaluation_process.next(evaluation_state, federated_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2eJ6gK67Ht3"
      },
      "outputs": [],
      "source": [
        "str(evaluation_output.metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcyt6yvwsywj"
      },
      "outputs": [],
      "source": [
        "evaluation_output.metrics[\"client_work\"][\"eval\"]['total_rounds_metrics']['true_positives']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAj2UM8EsT5S"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "tp = int(evaluation_output.metrics[\"client_work\"][\"eval\"]['total_rounds_metrics']['true_positives'])\n",
        "tn = int(evaluation_output.metrics[\"client_work\"][\"eval\"]['total_rounds_metrics']['true_negatives'])\n",
        "fp = int(evaluation_output.metrics[\"client_work\"][\"eval\"]['total_rounds_metrics']['false_positives'])\n",
        "fn = int(evaluation_output.metrics[\"client_work\"][\"eval\"]['total_rounds_metrics']['false_negatives'])\n",
        "\n",
        "cm = np.array([[tp, fp],\n",
        "               [fn, tn]])\n",
        "\n",
        "# labels\n",
        "labels = ['Positive', 'Negative']\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(cm)\n",
        "plt.title('Confusion matrix of the classifier')\n",
        "fig.colorbar(cax)\n",
        "ax.set_xticklabels([''] + labels)\n",
        "ax.set_yticklabels([''] + labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0XXv1dB7LEz"
      },
      "outputs": [],
      "source": [
        "federated_test_data = make_federated_data(test_data, test_data.client_ids)\n",
        "\n",
        "len(federated_test_data), federated_test_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apjmQG3w-3G2"
      },
      "outputs": [],
      "source": [
        "evaluation_output = evaluation_process.next(evaluation_state, federated_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZXj96PHRm1I"
      },
      "outputs": [],
      "source": [
        "tp = int(evaluation_output.metrics[\"client_work\"][\"eval\"]['total_rounds_metrics']['true_positives'])\n",
        "tn = int(evaluation_output.metrics[\"client_work\"][\"eval\"]['total_rounds_metrics']['true_negatives'])\n",
        "fp = int(evaluation_output.metrics[\"client_work\"][\"eval\"]['total_rounds_metrics']['false_positives'])\n",
        "fn = int(evaluation_output.metrics[\"client_work\"][\"eval\"]['total_rounds_metrics']['false_negatives'])\n",
        "\n",
        "cm = np.array([[tp, fp],\n",
        "               [fn, tn]])\n",
        "\n",
        "# labels\n",
        "labels = ['Positive', 'Negative']\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(cm)\n",
        "plt.title('Confusion matrix of the classifier')\n",
        "fig.colorbar(cax)\n",
        "ax.set_xticklabels([''] + labels)\n",
        "ax.set_yticklabels([''] + labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y1NAIkyuNEy"
      },
      "source": [
        "Deep learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeglu4fIuP8i"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "X = data.drop(columns=['Etat d\\'entropie', 'id'])\n",
        "y = data['Etat d\\'entropie']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_keras_model()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "# Test the model\n",
        "y_pred = (model.predict(X_test) > 0.9).astype(\"int32\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSz68X0ZvLLV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}